{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ded28c",
   "metadata": {},
   "source": [
    "# <font color='#000000'>Master Thesis Hugo Alves | <span style=\"color:#BFD62F;\">__Nova__</span> <span style=\"color:#5C666C;\">__IMS__</span></font>\n",
    "\n",
    "Welcome to the third chapter of notebooks developed for this project. We (Professor Roberto Henriques, Professor Ricardo Santos, and Hugo Alves) aim to develop a machine learning (ML) framework to predict student admissions to postgraduate and masters' programs at Nova IMS, as well as the final grade point average (GPA) of those who are accepted.\n",
    "\n",
    "# MANTER APENAS A IMAGEM CORRETA\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://i.ibb.co/tbdN6KX/Notebooks-Workflow.png\" alt=\"Workflow\" width=\"800\" />\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://i.ibb.co/BtH7KLZ/Notebooks-Workflow.png\" alt=\"Workflow\" width=\"800\" />\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "After exploring and preprocessing our data, we finally arrive to the __modelling__ stage. Thus, the main focus of this notebook will be to create, select, and test the models to predict admissions, as well as understanding which variables most contribute to these predictions. Enjoy!\n",
    "\n",
    "\n",
    "# <font color=\"#5C666C\">Contents (ADAPTAR!!!)</font> <a class=\"anchor\" id=\"toc\"></a>\n",
    "[Initial Setup](#setup)<br>\n",
    "- [Library and Functions Import](#library)<br>\n",
    "- [Retrieving the Dataframes](#dataframes)<br>\n",
    "   \n",
    "[4. Modelling](#modelling)<br>\n",
    "- [4.1. Decision Tree](#dt)<br>\n",
    "- [4.2. Logistic Regression](#lr)<br>\n",
    "- [4.3. Naïve Bayes](#nb)<br>\n",
    "- [4.4. Neural Networks](#nn)<br>\n",
    "- [4.5. Support Vector Machines](#svm)<br>\n",
    "- [4.6. Random Forest](#rf)<br>\n",
    "- [4.7. Bagging](#bagging)<br>\n",
    "- [4.8. Adaptive Boosting](#ab)<br>\n",
    "- [4.9. Gradient Boosting](#gb)<br>\n",
    "- [4.10. Stacking](#stacking)<br>\n",
    "- [4.11. Voting](#voting)<br>\n",
    "- [4.12. Final Model Selection](#selection)<br>\n",
    "\n",
    "[5. Evaluating variable importance](#shap)<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b0e50",
   "metadata": {},
   "source": [
    "# <font color=\"#BFD62F\">_____________</font>\n",
    "# <font color='#5C666C'>Initial Setup</font> <a class=\"anchor\" id=\"setup\"></a>\n",
    "[Back to Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b38df1",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>Library and Functions Import</font> <a class=\"anchor\" id=\"library\"></a>\n",
    "[Back to Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a204c7-1dbb-4e2c-94da-fce486715ff1",
   "metadata": {},
   "source": [
    "We are using the Python version 3.11.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1c5a41-f01c-4b7a-af37-68927406da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas==2.2.1\n",
    "#! pip install numpy==1.24.4\n",
    "#! pip install matplotlib==3.8.3\n",
    "#! pip install seaborn==0.12.2\n",
    "#! pip install plotly==5.20.0\n",
    "#! pip install tenacity==8.2.2\n",
    "#! pip install openpyxl>=3.1.0\n",
    "#% pip install nbformat>=4.3.0\n",
    "#! pip install rapidfuzz==3.11.0\n",
    "#! pip install xlrd==2.0.1\n",
    "#! pip install sklearn==1.2.2\n",
    "#! pip install imblearn==0.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6589c30-dc4d-40bb-85e3-10099c1ba040",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d92baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Functions as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d92d9",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>Retrieving the Dataframes </font> <a class=\"anchor\" id=\"dataframes\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "Let's retrieve the dataframes that we used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6289e570-d396-4287-9c94-404012489cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X_admissions_train_scaled\n",
    "%store -r X_admissions_val_scaled\n",
    "%store -r y_admissions_train\n",
    "%store -r y_admissions_val\n",
    "\n",
    "%store -r numerical_variables_admissions\n",
    "%store -r binary_variables_admissions\n",
    "%store -r categorical_variables_admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d8019-5c97-497c-82f9-9b7387c932c6",
   "metadata": {},
   "source": [
    "# ADAPTAR\n",
    "\n",
    "# <font color='#BFD62F'>______________</font>\n",
    "# <font color='#5C666C'>4. Modelling </font> <a class=\"anchor\" id=\"modelling\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "It's finally time to start making predictions on the students to admit to Nova IMS' postgraduate and masters' programs.\n",
    "\n",
    "We will start by testing some of the single models employed by the literature, namely the __Decision Tree__, __Logistic Regression__, __Naïve Bayes__, and __Support Vector Machines__. Although one reviewed study [(Chakraborty et al. (2018))](https://doi.org/10.1007/s12597-017-0329-2) used a hybrid model based on a decision tree and neural networks, and another [(Priyadarshini et al. (2023))](https://doi.org/10.1109/TransAI60598.2023.00040) opted for a deep learning approach, we will resort to a \"simpler\" __Artificial Neural Networks__ to aid us in our classification problem. <br>\n",
    "As for the ensemble models, we will utilize a __Random Forest__ to make predictions, as well as with __Bagging__, __Adaptive Boosting__, __Gradient Boosting__, __Stacking__, and __Voting__ algorithms (whose estimators will be determined through a grid search).\n",
    "\n",
    "For each algorithm, the ideal hyperparameters were determined through a grid search, which will hopefully return the closest to a global optimum of the model. We will utilize a function to fit the model to the training dataset and assess it on the validation data, returning accuracy, precision, recall, f1 score, and AUROC (where applicable) as metrics. It will also save the model to a Pickle file, so that it can easily be retrieved without constantly needing to run the function.\n",
    "\n",
    "__Note:__ To prevent running this notebook for a very long time, we will leave commented the code used for each algorithm. If needed, the models are stored in a Pickle file and can be called without the necessity to run the function again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712f219",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>4.1. Decision Tree </font> <a class=\"anchor\" id=\"dt\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "Decision trees are one of the simplest of ML algorithms. Essentially, they are a set of rules that leads some input data from the root node (the first in the tree) down to a leaf node, where all variables belonging to that leaf are assigned to a certain label. From the root node, all instances of the data are partitioned, using a value for a certain variable as a condition. All variables are tested, and the one that displays the highest predictive power is set as the condition for the root node. The data is split according to whether they match the winning condition, and the process is repeated for each of the new nodes that originated from the split. This procedure goes on until all samples inside the node contain the same label for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60480 candidates, totalling 302400 fits\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.15      0.24       808\n",
      "           1       0.82      0.97      0.89      3257\n",
      "\n",
      "    accuracy                           0.81      4065\n",
      "   macro avg       0.70      0.56      0.56      4065\n",
      "weighted avg       0.77      0.81      0.76      4065\n",
      "\n",
      "Accuracy: 0.8096 | Precision: 0.8215 | Recall: 0.9739 | F1 score: 0.8913 | AUROC: 0.7222\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dt_admissions = DecisionTreeClassifier(random_state = 92)\n",
    "\n",
    "dt_param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [None, 4, 6, 8, 10, 14, 18],\n",
    "    \"min_samples_split\": [0.01, 0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "    \"min_samples_leaf\": [0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5],\n",
    "    \"max_features\": [None, 0.1, 0.3, 0.5, \"sqrt\", \"log2\"],\n",
    "    \"max_leaf_nodes\": [None, 5, 10, 20, 30],\n",
    "    \"min_impurity_decrease\": [0, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "dt_best_params, dt_y_pred, dt_accuracy, dt_precision, dt_recall, dt_f1, dt_roc_auc = tf.run_model_classification(dt_admissions,\n",
    "                                                                                                                 dt_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Decision_Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa6ca5",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>4.2. Logistic Regression </font> <a class=\"anchor\" id=\"lr\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "Logistic regression is a statistical technique utilized to predict categorical outcomes from a set of independent variables when the dependent variable is categorical. Unlike linear regression, which is suited for problems where the target is numerical, logistic regression predicts the probability of belonging to a certain category, using the sigmoid function to ensure that the predicted values always fall between 0 and 1. It uses the maximum likelihood estimator to determine the coefficients that relate the predictors to the probability of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758ccf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1350 candidates, totalling 6750 fits\n",
      "\n",
      "Caching the list of root modules, please wait!\n",
      "(This will only be done once - type '%rehashx' to reset cache!)\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.11      0.20       808\n",
      "           1       0.82      0.99      0.89      3257\n",
      "\n",
      "    accuracy                           0.81      4065\n",
      "   macro avg       0.75      0.55      0.55      4065\n",
      "weighted avg       0.79      0.81      0.76      4065\n",
      "\n",
      "Accuracy: 0.8138 | Precision: 0.8179 | Recall: 0.9874 | F1 score: 0.8947 | AUROC: 0.7341\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_admissions = LogisticRegression(random_state = 92)\n",
    "\n",
    "lr_param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"C\": [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"saga\", \"sag\", \"newton-cholesky\"],\n",
    "    \"max_iter\": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "}\n",
    "\n",
    "lr_best_params, lr_y_pred, lr_accuracy, lr_precision, lr_recall, lr_f1, lr_roc_auc = tf.run_model_classification(lr_admissions,\n",
    "                                                                                                                 lr_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Logistic_Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4745d7",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>4.3. Naïve Bayes </font> <a class=\"anchor\" id=\"nb\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "Similarly to the logistic regression, Bayesian classification predicts class membership probabilities, with a threshold then assigning the predictions to a certain target label. It relies on the Bayes Theorem, which helps to determine the probability of an event with random knowledge, and is used to calculate the probability of one event, given that another event has already occurred. Essentially, it is the probability of a hypothesis given the presence of certain evidence. One of its key points (and the reason for it being \"naïve\") is that it assumes that the occurrence of an event is independent from the occurrence of other events - as such, each predictor individually contributes to a prediction, without relying on other independent variables.\n",
    "\n",
    "In our case, we will use a variation of Bayesian classification that assumes that the independent variables follow a Bernoulli distribution, which assumes them to be categorical. Although this is not true for all variables, we will still test it to assess its ability to generate accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3503645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       808\n",
      "           1       0.80      1.00      0.89      3257\n",
      "\n",
      "    accuracy                           0.80      4065\n",
      "   macro avg       0.40      0.50      0.44      4065\n",
      "weighted avg       0.64      0.80      0.71      4065\n",
      "\n",
      "Accuracy: 0.8012 | Precision: 0.8012 | Recall: 1.0 | F1 score: 0.8896 | AUROC: 0.4982\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nb_admissions = BernoulliNB()\n",
    "\n",
    "nb_param_grid = {\n",
    "    \"binarize\": np.arange(0, 1.1, 0.1)\n",
    "}\n",
    "\n",
    "nb_best_params, nb_y_pred, nb_accuracy, nb_precision, nb_recall, nb_f1, nb_roc_auc = tf.run_model_classification(nb_admissions,\n",
    "                                                                                                                 nb_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Naive_Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52525df",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>4.4. Neural Networks </font> <a class=\"anchor\" id=\"nn\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "Artificial neural networks are computing systems that attempt to mimic the animals' biological neural networks. It is composed of several perceptrons - single units of logic that output a binary conclusion - that can be organized in a number of layers through which the input data is passed.\n",
    "\n",
    "At the first iteration of a neural network, the input data is passed onto the first layer of nodes (perceptrons), which receives the name of input layer. Each value and variable of the input data are assigned a random weight, increasing or decreasing its relevance when it arrives to the input layer. Inside each node, an activation function transforms the input into an output, before passing it to the next layer (also with higher or lower random weights), where the process is repeated until the data arrives at the output layer, that returns a final prediction. The main goal is to optimize the weights and reduce the prediction error (loss), which is done through backpropagation and several iterations of the data passing through the nodes, where each should, ideally, be closer to an accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7efd6514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25920 candidates, totalling 129600 fits\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.19      0.29       808\n",
      "           1       0.83      0.97      0.89      3257\n",
      "\n",
      "    accuracy                           0.82      4065\n",
      "   macro avg       0.72      0.58      0.59      4065\n",
      "weighted avg       0.79      0.82      0.77      4065\n",
      "\n",
      "Accuracy: 0.8155 | Precision: 0.8282 | Recall: 0.9711 | F1 score: 0.894 | AUROC: 0.7464\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nn_admissions = MLPClassifier(random_state = 92)\n",
    "\n",
    "nn_param_grid = {\n",
    "    \"hidden_layer_sizes\": [(100,), (100, 100), (100, 50), (50,), (50, 50), (50, 25)],\n",
    "    \"activation\": [\"relu\", \"logistic\"],\n",
    "    \"solver\": [\"adam\", \"sgd\"],\n",
    "    \"alpha\": [0.00001, 0.0001, 0.0005, 0.001, 0.01, 0.1],\n",
    "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    \"batch_size\": [100, 200, 500],\n",
    "    \"learning_rate_init\": [0.0001, 0.001, 0.005, 0.01, 0.1],\n",
    "    \"max_iter\": [10, 50, 100, 200]\n",
    "}\n",
    "\n",
    "nn_best_params, nn_y_pred, nn_accuracy, nn_precision, nn_recall, nn_f1, nn_roc_auc = tf.run_model_classification(nn_admissions,\n",
    "                                                                                                                 nn_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Neural_Networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a10ca",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>4.5. Support Vector Machines </font> <a class=\"anchor\" id=\"svm\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "In SVM, each data item is seen as a point in the n-dimensional space. The classification task is done by finding the hyperplane that better differentiates the two (or more) target classes. Support vectors are simply the coordinates of an individual observation. A support vector machine is the frontier that best segregates the two classes. In regression problems, SVM finds the function that best fits the data within a specified margin of tolerance, with support vectors influencing the shape of the regression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 140 candidates, totalling 700 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svm_admissions = SVC(random_state = 92)\n",
    "\n",
    "svm_param_grid = {\n",
    "    \"C\": [0.01, 0.1, 0.5, 1, 10],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "svm_best_params, svm_y_pred, svm_accuracy, svm_precision, svm_recall, svm_f1, svm_roc_auc = tf.run_model_classification(svm_admissions,\n",
    "                                                                                                                        svm_param_grid,\n",
    "                                                                                                                        X_admissions_train_scaled,\n",
    "                                                                                                                        y_admissions_train,\n",
    "                                                                                                                        X_admissions_val_scaled,\n",
    "                                                                                                                        y_admissions_val,\n",
    "                                                                                                                        \"f1\",\n",
    "                                                                                                                        True,\n",
    "                                                                                                                        \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a800d",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>4.6. Random Forest </font> <a class=\"anchor\" id=\"rf\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "Ensemble learners rely on the combination of multiple supervised learning algorithms to make predictions. Following the Wisdom of the Crowds Principle, several weak learners might performed better as a whole, when compared to a strong individual learner.\n",
    "\n",
    "Random Forests are a group of decision trees trained in parallel, each using a sample of observations and a certain set of features from the feature space. In the end, their individual predictions are combined to obtain a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rf_admissions \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m92\u001b[39m)\n\u001b[0;32m      3\u001b[0m rf_param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m rf_best_params, rf_y_pred, rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrun_model_classification(rf_admissions,\n\u001b[0;32m     15\u001b[0m                                                                                                                  rf_param_grid,\n\u001b[0;32m     16\u001b[0m                                                                                                                  X_admissions_train_scaled,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m                                                                                                                  \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m                                                                                                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom_Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf_admissions = RandomForestClassifier(random_state = 92)\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [10, 20, 50, 75, 100, 200],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [None, 3, 6, 9, 12, 15],\n",
    "    \"min_samples_split\": [0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5],\n",
    "    \"min_samples_leaf\": [0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5],\n",
    "    \"max_features\": [None, 0.1, 0.3, 0.5, \"sqrt\", \"log2\"],\n",
    "    \"max_leaf_nodes\": [None, 5, 10, 20, 30],\n",
    "    \"max_samples\": [0.1, 0.2, 0.4, 0.6, 0.8, None]\n",
    "}\n",
    "\n",
    "rf_best_params, rf_y_pred, rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc = tf.run_model_classification(rf_admissions,\n",
    "                                                                                                                 rf_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Random_Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b56ae2",
   "metadata": {},
   "source": [
    "# REVER TEXTO E ABORDAGEM \n",
    "\n",
    "## <font color='#BFD62F'>4.7. Bagging </font> <a class=\"anchor\" id=\"bagging\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "Bootstrap aggregating (commonly known as bagging) is about parallelly training multiple instances of the same algorithm (estimator) on bootstrap (sampled with replacement) replicas of the training set, with each individual prediction being combined in the end to achieve a final prediction. The main focus of this technique is on reducing variance.\n",
    "\n",
    "As it may have been noticeable through the descriptions, random forest are bagging algorithms. However, in addition to bootstraping samples, random forests also do it with features, randomly selecting a subset of variables to include in each individual tree.\n",
    "\n",
    "__Note:__ In the function below, we are aware that when we use decision trees as estimators and the \"bootstrap_features\" parameter is set to true, we are in fact computing a random forest as well. The option to bootstrap features was included to assess the difference in performance of the remaining estimators, comparing a scenario where all features are included in all individual instances to one where a subset of variables is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514afd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bg_admissions = BaggingClassifier(random_state = 92)\n",
    "\n",
    "bg_param_grid = {\n",
    "    \"estimator\": [DecisionTreeClassifier(**dt_best_params, random_state = 92),\n",
    "                  MLPClassifier(**nn_best_params, random_state = 92),\n",
    "                  SVC(**svm_best_params, random_state = 92),\n",
    "                  RandomForestClassifier(**rf_best_params, random_state = 92)],\n",
    "    \"n_estimators\": [5, 10, 15, 20],\n",
    "    \"max_samples\": [0.1, 0.2, 0.4, 0.6, 0.8, 1],\n",
    "    \"max_features\": [0.1, 0.2, 0.4, 0.6, 0.8, 1],\n",
    "    \"bootstrap_features\": [True, False]\n",
    "}\n",
    "\n",
    "bg_best_params, bg_y_pred, bg_accuracy, bg_precision, bg_recall, bg_f1, bg_roc_auc = tf.run_model_classification(bg_admissions,\n",
    "                                                                                                                 bg_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Bagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafce9f6",
   "metadata": {},
   "source": [
    "# REVER TEXTO E ABORDAGEM\n",
    "\n",
    "## <font color='#BFD62F'>4.8. Adaptive Boosting </font> <a class=\"anchor\" id=\"ab\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "The idea behind boosting ensembles is that a weak learner can be boosted into a strong learning algorithm in a set of iterative training, where each new model is encouraged to pay more attention to the observations that were incorrectly classified by earlier models. A sequence of classifiers is created, where higher influence is given to the most accurate ones. The main focus of this technique is on reducing bias.\n",
    "\n",
    "In Adaptive Boosting (or AdaBoost, for short), a common approach is to create shallow decision trees as weak learners, known as stumps. Each stump considers the mistakes from previous stumps (the mistakes are given a higher weight when computing the following weak learner), and those who are more accurate are assigned a higher weight in the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ab_admissions = AdaBoostClassifier(random_state = 92)\n",
    "\n",
    "ab_param_grid = {\n",
    "    \"estimator\": [DecisionTreeClassifier(**dt_best_params, random_state = 92),\n",
    "                  MLPClassifier(**nn_best_params, random_state = 92),\n",
    "                  SVC(**svm_best_params, random_state = 92),\n",
    "                  RandomForestClassifier(**rf_best_params, random_state = 92)],\n",
    "    \"n_estimators\": [5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 1]\n",
    "}\n",
    "\n",
    "ab_best_params, ab_y_pred, ab_accuracy, ab_precision, ab_recall, ab_f1, ab_roc_auc = tf.run_model_classification(ab_admissions,\n",
    "                                                                                                                 ab_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Adaptive_Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccfae0",
   "metadata": {},
   "source": [
    "# REVER TEXTO E ABORDAGEM \n",
    "\n",
    "## <font color='#BFD62F'>4.9. Gradient Boosting </font> <a class=\"anchor\" id=\"gb\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "In contrast to AdaBoost, which assigns a higher weight to the observations that were incorrectly classified, Gradient Boosting uses the errors (residuals, in regression problems) to create a new, (ideally) improved weak learner. This process is performed until a certain threshold is achieved or the new instance of the model fails to improve the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909cb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gb_admissions = GradientBoostingClassifier(random_state = 92)\n",
    "\n",
    "gb_param_grid = {\n",
    "    \"estimator\": [DecisionTreeClassifier(**dt_best_params, random_state = 92),\n",
    "                  MLPClassifier(**nn_best_params, random_state = 92),\n",
    "                  SVC(**svm_best_params, random_state = 92),\n",
    "                  RandomForestClassifier(**rf_best_params, random_state = 92)],\n",
    "    \"n_estimators\": [10, 20, 50, 100],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.5, 1],\n",
    "    \"min_samples_split\": [0.01, 0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "    \"min_samples_leaf\": [0.01, 0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "    \"max_depth\": [None, 4, 6, 8, 10, 14, 20],\n",
    "    \"min_impurity_decrease\": [0, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "gb_best_params, gb_y_pred, gb_accuracy, gb_precision, gb_recall, gb_f1, gb_roc_auc = tf.run_model_classification(gb_admissions,\n",
    "                                                                                                                 gb_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Gradient_Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41c828",
   "metadata": {},
   "source": [
    "# REVER TEXTO E ABORDAGEM\n",
    "\n",
    "## <font color='#BFD62F'>4.10. Stacking </font> <a class=\"anchor\" id=\"stacking\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "# REVER E VER SE FICA MELHOR COM BAGGING BOOSTING\n",
    "\n",
    "Stacking (or stacked generalization) consists in stacking the output of individual estimators and using a meta-learning algorithm to compute the final predictions. First, a group of base models is created and used to make predictions, as they normally would; then, a meta-model compiles those predictions and performs a final one, given the knowledge provided by the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "st_admissions = StackingClassifier()\n",
    "\n",
    "# This is not a Grid Search, only put this way to fit into the function\n",
    "st_param_grid = {\n",
    "    \"estimators\": [[(\"Decision Tree\", DecisionTreeClassifier(**dt_best_params, random_state = 92)),\n",
    "                    (\"Logistic Regression\", LogisticRegression(**lr_best_params, random_state = 92)),\n",
    "                    (\"Neural Networks\", MLPClassifier(**nn_best_params, random_state = 92)),\n",
    "                    (\"SVM\", SVC(**svm_best_params, random_state = 92))]],\n",
    "    \"final_estimator\": [LogisticRegression(random_state = 92)]\n",
    "}\n",
    "\n",
    "st_best_params, st_y_pred, st_accuracy, st_precision, st_recall, st_f1, st_roc_auc = tf.run_model_classification(st_admissions,\n",
    "                                                                                                                 st_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Stacking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2c47e",
   "metadata": {},
   "source": [
    "# REVER TEXTO E ABORDAGEM\n",
    "\n",
    "## <font color='#BFD62F'>4.11. Voting </font> <a class=\"anchor\" id=\"voting\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "# ACRESCENTAR UM QUINTO ALGORITMO (E VERIFICAR WEIGHTS)\n",
    "\n",
    "As the name suggests, a voting algorithm aggregates the predictions of each weak learner, and opts for the one that was most commonly chosen for each observation.\n",
    "\n",
    "In our case, we will also test the possibility of assigning higher weights to the weak learners that performed better in terms of their f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_f1_scores = [dt_f1, lr_f1, nn_f1, svm_f1]\n",
    "\n",
    "sum_f1_scores = sum(voting_f1_scores)\n",
    "avg_f1_score = sum_f1_scores / len(voting_f1_scores)\n",
    "\n",
    "weights_f1 = []\n",
    "for i in voting_f1_scores:\n",
    "    added_weight = i - avg_f1_score\n",
    "    weight = 1 + 200 * added_weight\n",
    "    weights_f1.append(weight)\n",
    "\n",
    "weights_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vt_admissions = VotingClassifier()\n",
    "\n",
    "vt_param_grid = {\n",
    "    \"estimators\": [[(\"Decision Tree\", DecisionTreeClassifier(**dt_best_params, random_state = 92)),\n",
    "                    (\"Logistic Regression\", LogisticRegression(**lr_best_params, random_state = 92)),\n",
    "                    (\"Neural Networks\", MLPClassifier(**nn_best_params, random_state = 92)),\n",
    "                    (\"SVM\", SVC(**svm_best_params, random_state = 92))]],\n",
    "    \"weights\": [None, weights_f1]\n",
    "}\n",
    "\n",
    "vt_best_params, vt_y_pred, vt_accuracy, vt_precision, vt_recall, vt_f1, vt_roc_auc = tf.run_model_classification(vt_admissions,\n",
    "                                                                                                                 vt_param_grid,\n",
    "                                                                                                                 X_admissions_train_scaled,\n",
    "                                                                                                                 y_admissions_train,\n",
    "                                                                                                                 X_admissions_val_scaled,\n",
    "                                                                                                                 y_admissions_val,\n",
    "                                                                                                                 \"f1\",\n",
    "                                                                                                                 True,\n",
    "                                                                                                                 \"Voting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe1681",
   "metadata": {},
   "source": [
    "## <font color='#BFD62F'>4.12. Final Model Selection </font> <a class=\"anchor\" id=\"selection\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "We will now summarize our results in table before making a final selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4642df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Model\": [\"Decision Tree\", \"Logistic Regression\", \"Naïve Bayes\", \"Neural Networks\", \"SVM\",\n",
    "              \"Random Forest\", \"Bagging\", \"Adaptive Boosting\", \"Gradient Boosting\", \"Stacking\", \"Voting\"],\n",
    "    \"Accuracy\": [dt_accuracy, lr_accuracy, nb_accuracy, nn_accuracy, svm_accuracy,\n",
    "                 rf_accuracy, bg_accuracy, ab_accuracy, gb_accuracy, st_accuracy, vt_accuracy],\n",
    "    \"Precision\": [dt_precision, lr_precision, nb_precision, nn_precision, svm_precision,\n",
    "                  rf_precision, bg_precision, ab_precision, gb_precision, st_precision, vt_precision],\n",
    "    \"Recall\": [dt_recall, lr_recall, nb_recall, nn_recall, svm_recall,\n",
    "               rf_recall, bg_recall, ab_recall, gb_recall, st_recall, vt_recall],\n",
    "    \"F1 Score\": [dt_f1, lr_f1, nb_f1, nn_f1, svm_f1,\n",
    "                 rf_f1, bg_f1, ab_f1, gb_f1, st_f1, vt_f1],\n",
    "    \"ROC AUC\": [dt_roc_auc, lr_roc_auc, nb_roc_auc, nn_roc_auc, svm_roc_auc,\n",
    "                rf_roc_auc, bg_roc_auc, ab_roc_auc, gb_roc_auc, st_roc_auc, vt_roc_auc]\n",
    "}\n",
    "\n",
    "scores = pd.DataFrame(metrics)\n",
    "tf.highlight_max_column(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ff87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2f9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "24d3fc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>rf</th>\n",
       "      <th>xgb</th>\n",
       "      <th>lgbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prefix_1</th>\n",
       "      <td>0.573348</td>\n",
       "      <td>0.574910</td>\n",
       "      <td>0.560439</td>\n",
       "      <td>0.559533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_2</th>\n",
       "      <td>0.707851</td>\n",
       "      <td>0.689705</td>\n",
       "      <td>0.654775</td>\n",
       "      <td>0.703573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_3</th>\n",
       "      <td>0.639889</td>\n",
       "      <td>0.685729</td>\n",
       "      <td>0.656824</td>\n",
       "      <td>0.689930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_4</th>\n",
       "      <td>0.700765</td>\n",
       "      <td>0.731380</td>\n",
       "      <td>0.731635</td>\n",
       "      <td>0.737659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_5</th>\n",
       "      <td>0.735061</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.738774</td>\n",
       "      <td>0.737084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_6</th>\n",
       "      <td>0.810566</td>\n",
       "      <td>0.798742</td>\n",
       "      <td>0.823003</td>\n",
       "      <td>0.801288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_7</th>\n",
       "      <td>0.770016</td>\n",
       "      <td>0.761049</td>\n",
       "      <td>0.845399</td>\n",
       "      <td>0.820338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_8</th>\n",
       "      <td>0.804595</td>\n",
       "      <td>0.847170</td>\n",
       "      <td>0.871546</td>\n",
       "      <td>0.842686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_9</th>\n",
       "      <td>0.746369</td>\n",
       "      <td>0.805241</td>\n",
       "      <td>0.796150</td>\n",
       "      <td>0.818820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix_10</th>\n",
       "      <td>0.368622</td>\n",
       "      <td>0.368622</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.583976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dt        rf       xgb      lgbm\n",
       "prefix_1   0.573348  0.574910  0.560439  0.559533\n",
       "prefix_2   0.707851  0.689705  0.654775  0.703573\n",
       "prefix_3   0.639889  0.685729  0.656824  0.689930\n",
       "prefix_4   0.700765  0.731380  0.731635  0.737659\n",
       "prefix_5   0.735061  0.723577  0.738774  0.737084\n",
       "prefix_6   0.810566  0.798742  0.823003  0.801288\n",
       "prefix_7   0.770016  0.761049  0.845399  0.820338\n",
       "prefix_8   0.804595  0.847170  0.871546  0.842686\n",
       "prefix_9   0.746369  0.805241  0.796150  0.818820\n",
       "prefix_10  0.368622  0.368622  0.821865  0.583976"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_prefixes = 10\n",
    "models = ['dt', 'rf', 'xgb','lgbm']\n",
    "\n",
    "scores_dict = {model: [] for model in models}\n",
    "prefixes = [f'prefix_{i}' for i in range(1, num_prefixes + 1)]\n",
    "\n",
    "for i in range(1, num_prefixes + 1):\n",
    "    for model in models:\n",
    "        variable_name = f'final_score_{i}_{model}'\n",
    "        score = globals().get(variable_name, None)\n",
    "        scores_dict[model].append(score)\n",
    "\n",
    "scores_df = pd.DataFrame(scores_dict, index=prefixes)\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38dc22",
   "metadata": {},
   "source": [
    "The tables allows us to conclude regarding the best model for each prefix:\n",
    "* __Prefix 1__: Random Forest\n",
    "* __Prefix 2__: Decision Tree\n",
    "* __Prefix 3__: LightGBM\n",
    "* __Prefix 4__: LightGBM\n",
    "* __Prefix 5__: XGBoost\n",
    "* __Prefix 6__: XGBoost\n",
    "* __Prefix 7__: XGBoost\n",
    "* __Prefix 8__: XGBoost\n",
    "* __Prefix 9__: LightGBM\n",
    "* __Prefix 10__: XGBoost\n",
    "\n",
    "As we observe, simpler models work better with simpler datasets (as is the case for the first two prefixes), while the most complex ones perform better as we have more prefixes. Now, using these models, we will train the models on all data so that, when the project is implemented, the predictions are made taking into account the most recent information as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540c733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb377678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945c3741",
   "metadata": {},
   "source": [
    "# <font color='#BFD62F'>___________________________________</font>\n",
    "# <font color='#5C666C'>5. Evaluating Variable Importance </font> <a class=\"anchor\" id=\"shap\"></a>\n",
    "[Back to Contents](#toc)\n",
    "\n",
    "# LINKS ÚTEIS:\n",
    "* youtube: https://www.youtube.com/watch?v=L8_sVRhBDLU\n",
    "* (se por acaso não funcionar bem para as admissões por ser categórico): https://medium.com/towards-data-science/shap-for-categorical-features-7c63e6a554ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5e7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
